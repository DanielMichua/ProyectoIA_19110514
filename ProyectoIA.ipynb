{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProyectoIA",
      "provenance": [],
      "authorship_tag": "ABX9TyNR1uvJ5gyiNx1zLhd7BDKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielMichua/ProyectoIA_19110514/blob/main/ProyectoIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLNUDzsFSqnx"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install tflearn"
      ],
      "metadata": {
        "id": "kF8f4YvCStzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tflearn\n",
        "import random\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "with open('intents.json') as file:\n",
        "    intents = json.load(file, strict = False) \n",
        "intents = intents['intents'] \n",
        "\n",
        "print(\"[\", end = \"\")\n",
        "for intent in intents:\n",
        "  print(\"{\", end = \"\")\n",
        "  for key, value in intent.items():\n",
        "    print(\"{}: {},\".format(key, value))\n",
        "  print(\"\\b\\b\\n},\")\n",
        "print(\"\\b\\b]\")\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "retrain_model = True\n",
        "\n",
        "if retrain_model:\n",
        "    all_words = []\n",
        "    all_tags = [] \n",
        "    intent_patterns = [] \n",
        "    intent_tags = []\n",
        "    \n",
        "    for intent in intents:\n",
        "        for pattern in intent['patterns']:\n",
        "            words = nltk.word_tokenize(pattern)\n",
        "\n",
        "            all_words.extend(words)\n",
        "            intent_patterns.append(words)\n",
        "            intent_tags.append(intent['tag'])\n",
        "            \n",
        "        all_tags.append(intent['tag'])\n",
        "      \n",
        "  \n",
        "    all_words = [stemmer.stem(word.lower()) for word in all_words]\n",
        "    all_words = sorted(list(set(all_words)))\n",
        "    \n",
        "    all_tags = sorted(all_tags)\n",
        "    \n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    \n",
        "    y_empty = [0 for i in range(len(all_tags))]\n",
        "    \n",
        "    \n",
        "    for index, intent in enumerate(intent_patterns):\n",
        "        bag_of_words = []\n",
        "        \n",
        "        intent_words = [stemmer.stem(word.lower()) for word in intent]\n",
        "        \n",
        "        for word in all_words:\n",
        "            if word in intent_words:\n",
        "                bag_of_words.append(1)\n",
        "            else:\n",
        "                bag_of_words.append(0)\n",
        "                \n",
        "        one_hot_encode_y = y_empty[:]\n",
        "        one_hot_encode_y[all_tags.index(intent_tags[index])] = 1\n",
        "        \n",
        "        x_train.append(bag_of_words)\n",
        "        y_train.append(one_hot_encode_y)\n",
        "    \n",
        "\n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "    \n",
        "\n",
        "    with open('training_data.pickle', 'wb') as f:\n",
        "        pickle.dump((all_words, all_tags, x_train, y_train), f)\n",
        "else:\n",
        "    with open('training_data.pickle', 'rb') as f:\n",
        "        all_words, all_tags, x_train, y_train = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "DXgBNLaASvs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "neural_net = tflearn.input_data(shape = [None, len(x_train[0])])\n",
        "neural_net = tflearn.fully_connected(neural_net, 8)\n",
        "neural_net = tflearn.fully_connected(neural_net, 8)\n",
        "neural_net = tflearn.fully_connected(neural_net, len(y_train[0]), activation = 'softmax')\n",
        "neural_net = tflearn.regression(neural_net)\n",
        "\n",
        "model = tflearn.DNN(neural_net)\n",
        "\n",
        "if retrain_model:\n",
        "    model.fit(x_train, y_train, n_epoch = 500, batch_size = 8, show_metric = True)\n",
        "    model.save('model.tfl')\n",
        "else:\n",
        "    model.load('./model.tfl')\n"
      ],
      "metadata": {
        "id": "12J1rqo6S3fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_bag(text, all_words):\n",
        "    bag_of_words = [0 for i in range(len(all_words))]\n",
        "    \n",
        "\n",
        "    text_words = nltk.word_tokenize(text)\n",
        "    text_words = [stemmer.stem(word.lower()) for word in text_words]\n",
        "    \n",
        "\n",
        "    for word in text_words:\n",
        "        if word in all_words:\n",
        "            bag_of_words[all_words.index(word)] = 1\n",
        "    \n",
        "\n",
        "    return np.array(bag_of_words)\n",
        "\n",
        "\n",
        "\n",
        "def chat():\n",
        "    print(\"Enter a message to talk to the bot [type quit to exit].\")\n",
        "    \n",
        "    context_state = None\n",
        "    \n",
        "    default_responses = ['Sorry, Im not sure I know what you mean! You could try rephrasing that or saying something else!',\n",
        "                         'You confuse me human. Lets talk about something else.',\n",
        "                         'Im not sure what that means and I dont really care. Lets talk about something else',\n",
        "                         'I dont understand that! Try rephrasing or saying something else.']\n",
        "\n",
        "    while True:\n",
        "        user_chat = str(input('You: '))\n",
        "        if user_chat.lower() == 'quit':\n",
        "            break\n",
        "        \n",
        "        user_chat_bag = text_to_bag(user_chat, all_words)\n",
        "\n",
        "        response = model.predict([user_chat_bag])[0]\n",
        "\n",
        "        response_index = np.argmax(response)\n",
        "        response_tag = all_tags[response_index]\n",
        "        \n",
        "    \n",
        "        if response[response_index] > 0.8:\n",
        "            for intent in intents:\n",
        "                if intent['tag'] == response_tag:\n",
        "                    if 'context_filter' not in intent or 'context_filter' in intent and intent['context_filter'] == context_state:\n",
        "                        possible_responses = intent['responses']\n",
        "                        if 'context_set' in intent:\n",
        "                            context_state = intent['context_set']\n",
        "                        else:\n",
        "                            context_state = None\n",
        "                        print(random.choice(possible_responses))\n",
        "                    else:\n",
        "                        print(random.choice(default_responses))\n",
        "        else:\n",
        "            print(random.choice(default_responses))    "
      ],
      "metadata": {
        "id": "ngkFG5o_S9ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "id": "9VhmgvCSS_wy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}